# ESP 온디바이스 테스트

이 프로젝트는 ESP장치에서 구동되는 온디바이스 AI 모델을 구현하고 경량화 한 뒤, 가상 ESP에서 구동시키는 프로젝트입니다.

이 프로젝트의 구성은 다음과 같습니다.
1. 학습을 위한 데이터 수집
2. AI 아키텍쳐 구성, 학습, 경량화 모델 생성
3. ESP에 경량화된 모델 올리기
추가 - 샘플 오디오가 아닌 마이크로부터 입력받기

## 프로젝트 목표

이 프로젝트는 엑티브 노이즈 캔슬링 기능이 있는 무선 이어폰에서 영감을 받은 프로젝트입니다. 무선 이어폰의 내장된 마이크를 통해 주변의 소리를 듣고, 현재 사용자의 환경에 맞춰 노이즈 캔슬링을 실행시키는 작동 방식에서 현재 사용자의 환경을 인식하는 부분을 모방하려고 하였습니다.

## 1.데이터 수집

환경 소리를 분류하는 모델을 만들기 위해 우선 환경 소리 데이터를 수집하였습니다.
처음 구상한 AI 모델의 분류 결과는 [실내, 실외, 차량]이고, 이에 맞추어 오픈 데이터를 수집하였습니다.
사용한 데이터의 출처는 다음과 같습니다:

- https://github.com/karolpiczak/ESC-50
- https://zenodo.org/records/5606504

첫번째 데이터셋은 총 50가지 분류의 라벨된 2000개의 소리 데이터셋이고, 각 음성은 5초 이내의 길이이며, 16KHz로 샘플링 되었습니다.
두번째 데이터셋은 8가지의 분류의 차량 내부 소리 데이터셋이고, 각 음성은 5초 이내의 길이이며, 48KHz로 샘플링되었습니다.

우선적으로 학습을 위한 데이터 로더를 만들기 위해 두 데이터셋을 다시 실내, 실외, 차량, unknown 이라는 분류의 폴더로 다시 옮겼고, 또한 두번째 데이터셋을 리샘플링을 통해 16KHz로 낮추었습니다.
기기
위의 과정들은 prep_data.ipynb에 기록되었습니다.

## 2.모델 학습, 경량화 진행

앞서 1에서 수집된 데이터를 기반으로 데이터로더를 생성한뒤, 간단한 AI 모델 구조를 선언한 후 학습을 통해 AI 모델을 완성하였습니다.

AI 모델의 구조는 TFLite의 공식 튜토리얼에 나와있는 AI 모델의 구조를 가져와 크기를 더 줄이기 위해 FC레이어를 Conv레이어로 구조를 바꾸어 주었습니다.
AI 모델의 입력으로는 오디오 raw 입력을 쓰지 않고, 푸리에 변환을 통해 생성된 Mel Spectrogram을 입력으로 사용했습니다.

AI 모델의 구조는 다음과 같습니다:
InstanceNorm2d -> (Conv2d -> ReLU -> MaxPool2d) * 2 -> Linear

이후 AI 모델을 학습시키고, ESP에서 AI 구동에 사용되는 라이브러리인 TFLite Micro가 동적양자화를 지원하지 않기 때문에 ai_edge_torch와 TFLite를 통해 AI 모델에 완전정수 양자화를 적용시켜주었습니다.

위의 과정들은 prep_model.ipynb에서 진행되었습니다.

## 3.ESP에 경량화된 모델 올리기

2에서 완성된 경량화 모델과 ESP에서 제공하는 DSPS_FFT를 기반으로 ESP에 경량화된 AI 모델을 올렸습니다.

우선적으로 ESP를 직접 사용할 수 없는 환경에서 프로젝트가 진행되었기 때문에 QEMU를 통해 ESP에서 공식으로 지원하는 가상 ESP를 통해 테스트를 진행했습니다.

또한, 가상 ESP에서는 I2S가 지원되지 않기 때문에 음성 데이터가 아닌 사인 신호를 생성하여 테스트 입력으로 사용하였습니다.

ESP에서의 파이프라인은 다음과 같습니다:
음성 입력 -> melspectrogram -> AI 모델 -> 추론 결과 출력

## 3-1. 파일 설명

- generate_mel_bank.py - 멜 스펙트로그램생성을 위한 멜 필터뱅크를 생성하고 C로 저장하는 코드
- mel_filterbank.h - generate_mel_bank.py의 결과물.
- model_full_int8.tflite - 2에서 저장한 완전정수 양자화를 거친 경량화 모델
- model_data.h - xxd 를 통해 model_full_int8.tflite를 변환해 생성된 파일. 모델 파라미터를 C에서 읽을 수 있게 array의 형태로 저장되어있다.
- dsps_fft_main.cpp - 메인 함수 코드

템플릿 불러오기:

idf.py create-project-from-example "espressif/esp-dsp:fft"

디펜던시 설치:

idf.py add-dependency "espressif/esp-tflite-micro"

모델 변환 코드:

xxd -i your_model_name.tflite > model_data.h

## 추가 내용

프로젝트를 완성하기 위해선 실시간으로 마이크에서 받은 오디오 입력을 멜 스펙트로그램으로 변환한 뒤, AI 모델에 입력으로 넣어야 합니다.
오디오 입력을 받는 앞단의 구조입니다.

코드에 대한 설명들은 각 파일 내에 주석으로 추가하였습니다.